\chapter{P2P Content Delivery}
\section{Introduction}

Peer-to-peer (P2P) applications have attracted great attentions.
P2P applications such as Napster, Gnutella, FastTrack, BitTorrent, Skype and PPLive, have attracted the end users.
According to the P2P paradigm, the network of P2P is formed by peers that equally share the burden of providing services to each other in a cooperative manner.
Each peer contributes parts of its resources (network bandwidth, disk storage, etc.).
Unlike a client-server based system, P2P bring with them serving capacity. 
Therefore, as the nodes of P2P grows, the capacity of the network grows, too. 
This enables a P2P application is cheap to build and it can have good scalability for content delivery.  

We noted that the popular P2P file sharing applications are Gnutella, eDonkey, and Bittorrent.  
Gnutella is one of the earliest P2P file sharing.
Gnutella is one of those pure P2P applications that do not have a centralized server.
In initial phase connection, a Gnutella peer joins the network via at least one known peer, whose IP address is obtained either via an existing list of pre-configured addresses.
From this initial Gnutella peer, the new coming peer will discover more new Gnutella peers.
The source peer will send the search request to all actively connected peers. 
The recipient peer answers will answer the query if it knows anything useful, or forwards the request to other peers.
The query thus propagates among the Gnutella network.
Basically, Gnutella will floods the network to conduct the search and users will get content by doing search.

The eDonkey network operates as a hybrid P2P and server. 
The eDonkey network consists number of of clients and number of servers. 
The eDonkey server acts as a indexes files, communication hub, and for distributing IP addresses of other eDonkey servers to the eDonkey clients.

BitTorrent was created in 2002 by Bram Cohen. 
It runs on an open protocol specification thus there a lot of implementation.
To share a file or a set of files (bundle of files) through BitTorrent, a torrent file is first created. 
The torrent file contains the metadata of the shared content, which includes the information of the tracker that coordinates the file distribution, and the hashes of the file blocks to be distributed. 
The torrent file is usually distributed via web.
When a client wants to get a file, it first obtains the torrent file. 
The client then contacts the tracker listed in the torrent file to obtain a list of peers that are sharing the file at the same time.  
BitTorrent has generated great enthusiasm for P2P file sharing distribution.  
Many open source software projects use Bittorrent to distribute their applications.

Beside of P2P for filesharing, we also noted that P2P for streaming is also on the rise especially in China.
The target of P2P streaming is to build a scalable P2P platform for TV/music delivery. 
More than a dozen companies are actively working in this area for example UUSe, PPLive, PPStream. 
Let's take PPLive as an example since it is the most popular P2P streaming service.
According to \cite{} in 2007, the number of concurrent users for the most popular PPLive session raises to 1.5 million.
When the PPLive client is launched by users, it retrieves from a channel server the metadata information of all channels. 
The PPLive client presents the channel list to the user, who selects one particular channel to watch. 
After the channel selection, the PPLive client further talks to a tracker of that channel, and retrieves a list of peers that are watching the same channel. 
The PPLive client connects to a set of peers, and starts to exchange data. 
The challenge in P2P streaming is to provide a sustained streaming bit rate to all peers joining the network. 
Unlike P2P file sharing, where the content delivery can be carried out on a best effort basis, in P2P streaming, insufficient delivery bandwidth leads to poor quality of service, such as playback stalling or freezing, which is very annoying to the users.

\begin{figure}[tb]
\begin{center}
\includegraphics[scale=0.6]{../../../papers/p2p-cdn-latex/tex/ipsj/graphs/p2p-technologies.eps}
\end{center}
\caption{P2P technologies building block.} 
\label{fig:p2ptech}
\end{figure}

Figure \ref{fig:p2ptech} shows the presentation of content-centric P2P technologies.
It is not easy to identify an overall, clean, architecture in which the various P2P services related to content delivery. 
However, we can broadly group P2P services for content delivery applications in three main levels. T
he bottom level provides the basic networking abstractions, i.e., the P2P overlay networks. 
The middle level provides additional P2P services for content management and delivery. 
Above them, we can identify P2P applications. 
These can be either built directly on top of P2P overlay networks. 
Additional abstractions can be exist in the middle helping the content management and delivery process.
These abstractions may provide tree structures for meshes or gossip services, or even integrate P2P and CDN delivery mechanisms. 
P2P file sharing, streaming and Video-on-Demand (VoD) are typical examples of applications built on top of these abstractions. 
Finally, mobility, security, trust and cooperation are seen as cross-layer issues,.
Problems related to mismatches between the traffic generated by P2P applications and peering agreements among ISPs, which are also in a cross-layer issue.

All P2P networks run on top of the Internet. 
We often consider the P2P network as an overlay network.
We classified overlay to: structured, unstructured, and hierarchical. 
The general idea of structured overlays is defining a virtual address space and assign overlay addresses to peers in this space which defines neighborhood relationships between peers.
The most common use of such an overlay is for storing and locating objects. 
An object to be stored in the overlay is stored on the peer responsible for the address of the object.
The core functions of a structured overlay network are address assignment, join and leave operations, structure maintenance, routing and forwarding.

Unlike structured overlays, unstructured systems do not enforce any particular structure in the network. 
From a topological standpoint, unstructured networks typically resemble random graphs. 
In some unstructured P2P networks peers do have not all the same role. 
The basic mechanism used to build unstructured networks permits several degrees of freedom in the choice of the neighbors. 
This can be exploited to obtain certain properties in the overlay structure. 
Moreover, a few proposals look at choosing neighbors based on the users interests. 
The rationale behind this choice is that people always sharing common interests are likely to share similar content, and thus searches for a particular type of content is more efficient if peers likely to store that content type are neighbors. 

Conceptually, unstructured architectures with superpeers are hierarchical networks, the superpeers forming the top level, and ordinary peers the bottom level.
However, this form of overlays is usually still classified in the unstructured family. 
With hierarchical overlays we thus denote a more complex architecture. 
Nodes are organized in different groups, and each group runs its own overlay. 
A top-level overlay is then formed by one representative per group. 

Searching for content is one of the key services for which overlay networks are used, and thus it has attracted  and still attracts  a lot of attention.
In the area of P2P networks, indices can be of three types: centralized, localized, or distributed. 
Centralized indices typically rely on a unique entity in the network that stores the index. 
Peers have to query the central index in order to know where the content of interest is stored.
Using localized indices is what most unstructured networks do. 
In this case, each peer indexes only content available locally. 
A query looking for a particular content must thus be propagated among peers in order to check each local index.
However, indexing does not solve content poisoning.

Search techniques in unstructured networks are very widely interested to researchers. 
One of the reasons is that unstructured topologies are very robust with respect to flash crowd, making them suitable for distributed content retrieval systems. However, searching typically results in the exploration of the overlay. 
Making such exploration efficient in terms of overhead, without sacrificing performance in terms of success ratio, is an exciting and non-trivial problem.
The search techniques described so far assume that pieces of content are replicated as peers request, find and fetch them on the local nodes. 
Another important direction focuses on search driven proactive replication, i.e. looks at how to proactively replicate content in order to optimize the search functionality. 
This body of work is actually shared between search and replication techniques. 

Structured overlays provide the perfect support for exact match queries, i.e., queries issued on the values of keys used for storing the contents in the overlay. 
In these cases users query for content whose key is k, and, due to the overlay mechanism, the only place where the content can be is at the peer responsible for the address hash(k). 
Therefore, a search operation results in a forwarding operation, and is as efficient as forwarding on the overlay.
The challenge in structured networks is providing efficient free-text (or keyword) search, which is a much more general case in which users asks for any piece of content containing a given set of keywords. 
Note that in unstructured networks this type of queries is easily supported, essentially because indices are mostly local. 
For example, when random walks are used, each peer visited by the random walk can easily lookup the index for content having the set of keywords K.

Replication solutions are typically tied with the underlying P2P overlays they are designed for. 
In the case of structured P2P networks, a typical approach consists in exploiting closeness in the ID space to proactively replicate objects. 
Caching is another key topic in distributed systems and therefore in P2P systems. 
However, no particularly original solutions have been proposed with specific reference to P2P environments.
As a complement to replication policies, researchers have also investigated consistency solutions for P2P environments. 
Clearly, replication and consistency management are intertwined topics in distributed systems. 
Consistency management is an extremely well investigated topic in the caching area.

Gossiping is a simple yet effective mechanism with numerous applications in various fields of distributed computing. 
It builds upon a very simple mechanism to distribute a generic piece of information in a distributed system. 
Each peer participating in a gossiping process contacts a random number of other peers it is aware of (the number of peers, also known as fanout, being a system parameter). 
In push gossip, the contacted peers receive a piece of information from the contacting peer. 
In pull gossip, the contacted peers send a piece of information to the contacting peer. 
The set of peers to contact can be chosen completely at random, or according to some preferential rule.
Gossip mechanisms have also been used as underlying services to build both structured and unstructured networks. 

Another important facet of security for P2P networks is clearly related to trust, and particularly to establishment of reputation. 
Typically, establishing reputation of peers requires collecting information about previous interactions, distinguishes between probabilistic estimation and social-based estimation. 
Reputation is established based on a restricted sample of the complete information about previous interactions. 
It aggregates of the whole available information are used to establish trustworthiness.
Finally, cooperation enforcement are other key topics related to security in P2P environments. T
The tit-for-tat policy of BitTorrent is one popular example of cooperation enforcement.

We highlight an important aspect related to the impact of P2P technologies on ISP policies (cross layer issues). 
P2P solutions are most of the time network agnostic, in the sense that they do not take in great consideration the physical characteristics of the network paths they generate. 
Having agnostic P2P solutions can cause problems at different levels to ISPs. 
It has been shown that blind selections of neighbors in P2P networks may result in unnecessary traversal of multiple links inside an ISP. 
Furthermore, it can significantly impact on the shape and amount of traffic among different ISPs, which may result being quite different from what foreseen in ISPs peering agreements.
Although, many solution exists the implementations are still not deployed. 

While algorithms and mechanism in P2P system has been analyzed by many researchers, measurement of P2P is other aspect that important to do. 
For example, in cross layer issue, P2P is often blind in selection of neighbors in P2P networks that affect underlying ISP's traffic engineering policies.  
This effect can only seen if we have good real measurement instead doing simulation.






\section{P2P Measurement}


Bittorrent is the most successful Peer-to-Peer (P2P) application and is responsible for a major portion of Internet traffic. 
This has attracted the interest of the research community that has thoroughly evaluated the performance and the demographic aspects of BitTorrent. 
It has been largely studied using simulations, models and real measurements. 
Although simulations and modelling are easier to perform, they typically simplify analyzed problems and in case of Bittorrent they are likely to miss some of the effects which occur in real swarms. 
Due to the complexity of the system, the most relevant studies have tried to understand different aspects by performing real measurements of BitTorrent swarms in the wild, this is inferring information from real swarms in real time.
Several techniques have been used in order to measure different aspects of BitTorrent so far. 
We present different measurement techniques that constitutes a first step in the designing the future measurement techniques and tools for analyzing large scale systems.

\section{Measuring BitTorrent}
In this Section we describe the BitTorrent measurement techniques defined in the literature so far. 
We classify them into two main categories macroscopic and microscopic depending on the retrieved information. 

\subsection{Macroscopic Technique}
The main objective of these techniques is to understand the demographics of the BitTorrent ecosystem: the type of published content, the popularity of the content, the distribution of BitTorrent users per country (or ISP), the relevance of the different portals and trackers, etc. 
Furthermore, the macroscopic measurements allow to study some performance aspects such as the ratio of seeders/leechers, the session time of the BitTorrent users, the arrival rate of peers, the seedless state (period the torrent is without seeder) duration, etc.
We classify the macroscopic techniques into two subcategories: BitTorrent portals crawling and BitTorrent trackers crawling.

\subsection{Microscopic Technique}
The described macroscopic techniques retrieve exclusively the peers IP addresses, thus only metrics associated to the presence/absence of the peer can be studied. Unfortunately, IP address does not suffice to infer relevant performance metrics at the peer level such as peers’ download and upload rate. 
For this purpose we need to apply more sophisticated (but less scalable) techniques that we name microscopic techniques.
To perform microscopic techniques we need to implement different parts of the BitTorrent peer wire protocol. 
Any microscopic crawler has to implement the functions to perform the handshaking procedure. 
This is essential to connect to other peers. 
The handshaking procedure can be done actively (the crawler initiates it) or passively (the crawler waits until a peer starts the handshaking). 
Once the crawler is connected to a peer, it exploits different messages of the peer wire protocol in order to measure different parameters. 


\section{Green Networking}

Since the seminal paper by Gupta and Singh \cite{Gupta:2003:GI:863955.863959}, presented at SIGCOMM in 2003, the subject of green networking has received considerable attention. 
In recent years, valuable efforts have been devoted to reducing unnecessary energy expenditure.
Big companies such as Google, Microsoft, and Amazon, are turning to a host of new technologies to reduce operating costs and consume less energy.
Google, for example, is planning to operate its data centers with a zero carbon footprint by using, among other things, hydropower, water-based chillers, and external cold air to do some of the cooling.
Several approaches have been considered to reduce energy consumption in networks. These include:
\begin{itemize}
	\item The design of low power components that are still able to offer acceptable levels of performance. 
	For example, at the circuit level techniques such as Dynamic Voltage Scaling (DVS) and Dynamic Frequency Scaling (DFS) can be used. 
	With DVS the supply voltage is reduced when not needed, which results in slower operation of the circuitry. 
	DFS reduces the number of processor instructions in a given amount of time, thus reducing performance. 
	These techniques can reduce energy consumption significantly. 
	
	\item Consuming energy from renewable energy sources sites rather than incurring in electricity transmission overheads, thus reducing CO2 emissions.
	
	\item Designing new network architectures, for example by moving network equipment and network functions to strategic places. 
	Examples include placing optical amplifiers at the most convenient locations and performing complex switching and routing functions near renewable sources.
	
	\item Using innovative cooling techniques. Researchers in Finland, for instance, are running servers outside in Finnish winter, with air temperatures below $20$ degrees celsius.
	
	\item Performing resource consolidation, capitalizing on available energy. 
	This can be done via traffic engineering, for instance. 
	By aggregating traffic flows over a subset of the network devices and links allows others to be switched off temporarily or be placed in sleep mode. 
	Another way is by migrating computation, typically using virtualization to move workloads transparently.
	
\end{itemize}
